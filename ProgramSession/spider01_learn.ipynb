{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是爬虫\n",
    "\n",
    "爬虫：一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "\n",
    "> 通过编程向网络服务器请求数据（HTML表单），然后解析HTML，提取出自己想要的数据\n",
    "\n",
    "\n",
    "![](\\assets\\spider_01.jpg)\n",
    "\n",
    "归纳为四大步：\n",
    "\n",
    "- 根据url获取HTML数据\n",
    "- 解析HTML，获取目标信息\n",
    "- 存储数据\n",
    "- 重复第一步\n",
    "\n",
    "这会涉及到数据库、网络服务器、HTTP协议、HTML、数据科学、网络安全、图像处理等非常多的内容。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 什么是HTML\n",
    "\n",
    "\n",
    "HTML 是整个网页的结构，相当于整个网站的框架。带“＜”、“＞”符号的都是属于 HTML 的标签，并且标签都是成对出现的。\n",
    "\n",
    "常见的标签如下：\n",
    "\n",
    "```python\n",
    "<html>..</html> 表示标记中间的元素是网页\n",
    "<body>..</body> 表示用户可见的内容\n",
    "<div>..</div> 表示框架\n",
    "<p>..</p> 表示段落\n",
    "<li>..</li>表示列表\n",
    "<img>..</img>表示图片\n",
    "<h1>..</h1>表示标题\n",
    "<a href=\"\">..</a>表示超链接\n",
    "```\n",
    "**CSS**  表示样式，在 CSS 中定义了外观。\n",
    "**JScript** 表示功能。交互的内容和各种特效都在 JScript 中，JScript 描述了网站中的各种功能。\n",
    "\n",
    "如果用人体来比喻，HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里。CSS 是人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等。JScript 表示人的技能，例如跳舞、唱歌或者演奏乐器等。\n",
    "\n",
    "\n",
    ">HTML (HyperText Markup Language) is the most basic building block of the Web. It defines the meaning and structure of web content. Other technologies besides HTML are generally used to describe a web page's appearance/presentation (CSS) or functionality/behavior (JavaScript).\n",
    "\n",
    "一个HTML的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于爬虫的合法性\n",
    "\n",
    "几乎每一个网站都有一个名为 robots.txt 的文档，当然也有部分网站没有设定 robots.txt。对于没有设定 robots.txt 的网站可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。如果网站有 robots.txt 文档，就要判断是否有禁止访客获取的数据。\n",
    "\n",
    "https://xueqiu.com/robots.txt\n",
    "\n",
    "\n",
    "User-agent: * 代表的所有的搜索引擎种类，\n",
    "\n",
    "Disallow: /admin/ 这里定义是禁止爬寻admin目录下面的目录\n",
    "\n",
    "Disallow: /require/ 这里定义是禁止爬寻require目录下面的目录\n",
    "\n",
    "Disallow: /ABC/ 这里定义是禁止爬寻ABC目录下面的目录\n",
    "\n",
    "Disallow: /cgi-bin/*.htm 禁止访问/cgi-bin/目录下的所有以”.htm”为后缀的URL(包含子目录）。\n",
    "\n",
    "Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址\n",
    "\n",
    "Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片\n",
    "\n",
    "Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 requests 库请求网站\n",
    "安装 requests 库\n",
    "\n",
    "pip install requests\n",
    "\n",
    "\n",
    "#### 爬虫的基本原理\n",
    "\n",
    "网页请求的过程分为两个环节：\n",
    "- Request （请求）：每一个展示在用户面前的网页都必须经过这一步，也就是向服务器发送访问请求。\n",
    "- Response（响应）：服务器在接收到用户的请求后，会验证请求的有效性，然后向用户（客户端）发送响应的内容，客户端接收服务器响应的内容，将内容展示出来，就是我们所熟悉的网页请求.\n",
    "\n",
    "\n",
    "网页请求的方式也分为两种：\n",
    "- GET：最常见的方式，一般用于获取或者查询资源信息，也是大多数网站使用的方式，响应速度快。\n",
    "- POST：相比 GET 方式，多了以表单形式上传参数的功能，因此除查询信息外，还可以修改信息。\n",
    "\n",
    "所以，在写爬虫前要先确定向谁发送请求，用什么方式发送。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        #导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "res = requests.get(url)        #Get方式获取网页数据\n",
    "\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么要设置headers?\n",
    "\n",
    "在请求网页爬取的时候，输出的text信息中会出现抱歉，无法访问等字眼，这就是禁止爬取，需要通过反爬机制去解决这个问题。\n",
    "\n",
    "headers是解决requests请求反爬的方法之一，相当于我们进去这个网页的服务器本身，假装自己本身在爬取数据。\n",
    "\n",
    "对反爬虫网页，可以设置一些headers信息，模拟成浏览器取访问网站 。\n",
    "\n",
    "#### headers 哪里找\n",
    "\n",
    "谷歌或者火狐浏览器，在网页面上点击：右键–>检查\n",
    "\n",
    "headers中有很多内容，主要常用的就是user-agent 和 host，他们是以键对的形式展现出来，如果user-agent 以字典键对形式作为headers的内容，就可以反爬成功，就不需要其他键对；否则，需要加入headers下的更多键对形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        #导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "res = requests.get(url, headers = headers)        #Get方式获取网页数据\n",
    "res.encoding = 'utf-8'  # 保证中文的显示\n",
    "# print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'errno': 998, 'errmsg': '未知错误', 'query': '中国银行', 'from': 'zh', 'to': 'en', 'error': 998}\n"
     ]
    }
   ],
   "source": [
    "#### 使用POST 方式\n",
    "\n",
    "import requests        #导入requests包\n",
    "import json\n",
    "\n",
    "def get_translate(word):\n",
    "    # General Request URL\n",
    "    url = 'https://fanyi.baidu.com/v2transapi?from=zh&to=en'\n",
    "    form_data = {'from':'zh', 'to':'en', 'query':word, 'transtype':'translang', 'simple_means_flag':'3', 'sign':'777849.998728', 'token':'8fdc86c1912abf9a6792ab0df40760c5', 'domain':'common'}\n",
    "    headers = {\n",
    "        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36',\n",
    "        'Cookie':'BIDUPSID=BD1F6031E6E1F58EBA5780A6882450DB; PSTM=1511924987; BDUSS=2F1RlhGZEZGZzJFOE1RZExnazN2bWZTTWc4aWswVWRnOTFKUWh3UjA1MVJZblpiQUFBQUFBJCQAAAAAAAAAAAEAAACIDXkhbmloZTc4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFHVTltR1U5bZ; H_WISE_SIDS=139560_141910_100805_142081_142208_142066_135847_141001_138596_140853_141916_142002_137758_138878_137985_141200_140173_131246_137746_138165_107319_138883_140260_141838_140632_139043_140202_140592_136861_138585_141651_140988_141900_140113_140324_140579_133847_131423_140367_140965_136537_141102_110085_141941_127969_140593_131953_139887_140995_138425_138943_141190_141924; BDUSS_BFESS=2F1RlhGZEZGZzJFOE1RZExnazN2bWZTTWc4aWswVWRnOTFKUWh3UjA1MVJZblpiQUFBQUFBJCQAAAAAAAAAAAEAAACIDXkhbmloZTc4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFHVTltR1U5bZ; delPer=0; PSINO=3; ZD_ENTRY=bing; BDRCVFR[feWj1Vr5u3D]=I67x6TjHwwYf0; session_name=cn.bing.com; MCITY=-%3A; BAIDUID=32A052A10603308B16C560D6C9D060BE:FG=1; BAIDUID_BFESS=F170A5773764B336959E01B66D608AFB:FG=1; session_id=1604470913738; H_PS_PSSID=1424_33043_32947_33059_31253_32971_32706_32961_32846; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1605020596; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1605020596; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; __yjsv5_shitong=1.0_7_50d6a0d85be4941d2e489f60c1c0ce1e8ce8_300_1605020594794_124.160.64.90_b1c9306c; yjs_js_security_passport=64e43975da70565c859cacfcc9d315009e61e875_1605020596_js'\n",
    "        }\n",
    "    #请求表单数据\n",
    "    response = requests.post(url,data=form_data, headers=headers)\n",
    "    #将Json格式字符串转字典\n",
    "    content = json.loads(response.text)\n",
    "    print(content)\n",
    "\n",
    "\n",
    "get_translate('中国银行')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup\n",
    "\n",
    "就是一个第三方的库，使用之前需要安装\n",
    "\n",
    "pip install bs4\n",
    "pip install lxml\n",
    "\n",
    "- bs4是什麽？\n",
    "\n",
    "它的作用是能够快速方便简单的提取网页中指定的内容，给我一个网页字符串，然后使用它的接口将网页字符串生成一个对象，然后通过这个对象的方法来提取数据\n",
    "\n",
    "- lxml是什麽？\n",
    "\n",
    "lxml是一个解析器，也是下面的xpath要用到的库，bs4将网页字符串生成对象的时候需要用到解析器，就用lxml，或者使用官方自带的解析器 html.parser\n",
    "\n",
    "**一般步骤：**\n",
    "1. 通过requests库爬取html页面的内容\n",
    "2. 使用BeautifulSoup库对爬取到的html页面进行解析\n",
    "3. 使用BeautifulSoup以及正则表达式来进一步提取我们想要的关键信息\n",
    "4. 将信息格式化并输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input \"password\": 111\n"
     ]
    }
   ],
   "source": [
    "# SINA 爬虫实例\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen,Request\n",
    "from urllib.error import URLError,HTTPError\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "conn_sql = 'mysql+mysqldb://root:{}@127.0.0.1:3306/news_online?charset=utf8'.format(input('Please input \"password\":'))\n",
    "conn = create_engine(conn_sql)  \n",
    "\n",
    "def html_download(url):\n",
    "     headers = {\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/53'\n",
    "            }\n",
    "     request = Request(url,headers = headers)\n",
    "     try:\n",
    "         html = urlopen(request).read().decode()\n",
    "     except HTTPError as e:\n",
    "         html = None\n",
    "         print('请求服务器出错：%s'%e.reason)\n",
    "         return None\n",
    "     except URLError as e:\n",
    "         html = None\n",
    "         print('请求网页出错：%s'%e.reason)\n",
    "         return None\n",
    "     return html\n",
    " \n",
    "def json2df(json_results):\n",
    "    res = pd.DataFrame.from_records(json_results)\n",
    "    tags = []\n",
    "    for r in res.iterrows():\n",
    "        try:\n",
    "            tags.append(r[-1]['tag'][0]['name'])\n",
    "        except:\n",
    "            tags.append('其他')\n",
    "    x = res.loc[:,['id','commentid','creator','rich_text','update_time','zhibo_id']]\n",
    "    x['tag'] = tags\n",
    "    return x\n",
    " \n",
    "def api_info_manager(page, zhibo_id = 152):\n",
    "    #http://zhibo.sina.com.cn/api/zhibo/feed?&page=1&page_size=100&zhibo_id=152\n",
    "    data = {\n",
    "            'page':page,\n",
    "            'page_size':100,\n",
    "            'zhibo_id':zhibo_id\n",
    "            }\n",
    "    dataformat = 'http://zhibo.sina.com.cn/api/zhibo/feed?' + urlencode(data)\n",
    "    response = html_download(dataformat)\n",
    "    return json.loads(response,encoding = 'utf-8')['result']['data']['feed']['list']\n",
    "    #json_results = json.dumps(json_results,ensure_ascii = False)\n",
    "    #print(json_results)\n",
    "\n",
    "        \n",
    "def save_to_sql(res):\n",
    "    try:\n",
    "        r = res.sort_values(by='id', ascending = True)\n",
    "        # You need a database named news_online \n",
    "        pd.io.sql.to_sql(r,'sina_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "    except Exception:\n",
    "        print('Fail')\n",
    "\n",
    "def update_sql(res):\n",
    "    try:\n",
    "        last_id = int(pd.read_sql_query('select id from sina_fin_news ORDER BY id desc LIMIT 1', conn).id)\n",
    "        in_list = res[res['id']>last_id]\n",
    "        new_l = in_list.sort_values(by='id', ascending = True)\n",
    "        pd.io.sql.to_sql(new_l,'sina_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "       \n",
    "        print('{} items has been update Successed on {}'.format(len(new_l), time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    except Exception:\n",
    "        \n",
    "        print('Fail to update on {}'.format(time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "def main(page):\n",
    "    json_res = api_info_manager(page)\n",
    "    res = json2df(json_res)\n",
    "    save_to_sql(res)\n",
    "\n",
    "def updating():\n",
    "    while True:\n",
    "        json_res = api_info_manager(1)\n",
    "        res = json2df(json_res)\n",
    "        update_sql(res)\n",
    "        time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_res = api_info_manager(1)\n",
    "res = json2df(json_res)\n",
    "# res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input \"username:password\": 222\n"
     ]
    }
   ],
   "source": [
    "### 雪球 \n",
    "\n",
    "from urllib.parse import urlencode\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "import pandas as pd  \n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "\n",
    "conn_sql = 'mysql+mysqldb://root:{}@10.23.0.2:3306/news_online?charset=utf8'.format(input('Please input password:'))\n",
    "conn = create_engine(conn_sql)  \n",
    "\n",
    "\n",
    "\n",
    "def getcookies():#获得雪球网的cookie\n",
    "    headers3 = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "           'Referer': 'https://xueqiu.com/today',\n",
    "           'Host': 'xueqiu.com',\n",
    "           }\n",
    "    r = requests.get(url = 'https://xueqiu.com/', headers=headers3)\n",
    "    if r.status_code == 200:\n",
    "        cookie = r.cookies.get_dict()\n",
    "        return cookie\n",
    "    return None\n",
    "\n",
    "def html_download(url, cookie):\n",
    "    headers3 = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "               'referer': 'https://xueqiu.com/',\n",
    "               'Host': 'xueqiu.com',\n",
    "               }\n",
    "    try:\n",
    "        request = requests.get(url, headers=headers3,cookies=cookie)\n",
    "        if request.status_code == 200:\n",
    "            return request.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "\n",
    "def api_info_manager(cookie):\n",
    "    data = {\n",
    "            'since_id': -1,\n",
    "            'max_id': -1,\n",
    "            'count': 20,\n",
    "            'category': 6\n",
    "            }\n",
    "    dataformat = 'https://xueqiu.com/v4/statuses/public_timeline_by_category.json?' + urlencode(data)\n",
    "    response = html_download(dataformat,cookie)\n",
    "    if not response: \n",
    "        # if not works, do it again\n",
    "        cookie = getcookies()\n",
    "        response = html_download(dataformat,cookie)\n",
    "    \n",
    "    info = json.loads(response, encoding='utf-8')['list']\n",
    "    x = pd.DataFrame(columns =['id','category','text','target','view_count','created_at'])\n",
    "    for i,info in enumerate(json.loads(response, encoding='utf-8')['list']):\n",
    "        x.loc[i,'id'] = info['id']\n",
    "        x.loc[i,'category'] = info['category']\n",
    "        x.loc[i,'text'] = json.loads(info['data'])['text']\n",
    "        x.loc[i,'target'] = json.loads(info['data'])['target']\n",
    "        x.loc[i,'view_count'] = json.loads(info['data'])['view_count']\n",
    "        x.loc[i,'created_at'] = datetime.datetime.fromtimestamp(int(json.loads(info['data'])['created_at']/1000)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return x        \n",
    "     \n",
    "\n",
    "def update_sql(res):\n",
    "    try:\n",
    "        last_id = int(pd.read_sql_query('select id from xueqiu_fin_news ORDER BY id desc LIMIT 1', conn).id)\n",
    "        in_list = res[res['id']>last_id]\n",
    "        new_l = in_list.sort_values(by='id', ascending = True)\n",
    "        pd.io.sql.to_sql(new_l,'xueqiu_fin_news', con=conn, schema='news_online', if_exists = 'append')\n",
    "        print('{} items has been update Successed on {}'.format(len(new_l), time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    except Exception:\n",
    "        print('Fail to update on {}'.format(time.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    \n",
    "def main(page):\n",
    "    cookie = getcookies()\n",
    "    res = api_info_manager(cookie)\n",
    "    update_sql(res)\n",
    "\n",
    "def updating(cookie):  \n",
    "    while True:\n",
    "        res = api_info_manager(cookie)\n",
    "        update_sql(res)\n",
    "        time.sleep(3600)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #cookie = getcookies()\n",
    "    #updating(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
